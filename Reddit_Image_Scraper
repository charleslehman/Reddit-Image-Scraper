#!/usr/bin/env perl

#   Reddit Image Scraper: A perl script to download images hosted on
#   the imgur.com hosting service linked from a subreddit at reddit.com
#   Copyright (C) 2013 Joshua Hull

#   This program is free software: you can redistribute it and/or modify
#   it under the terms of the GNU General Public License as published by
#   the Free Software Foundation, either version 3 of the License, or
#   (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program.  If not, see <http://www.gnu.org/licenses/>.

use strict;
use warnings;

use JSON;
use WWW::Mechanize;
use URI;
use Getopt::Std;

use constant INTERVAL => 2;

my $version_string = "2013.10.18";

my $user_agent = "Reddit_Image_Scraper/".$version_string." by /u/hyperspace290";

my $mech = WWW::Mechanize->new(autocheck=>0,agent=>$user_agent);

$Getopt::Std::STANDARD_HELP_VERSION = 1;
getopts('ul:q');
our($opt_u,$opt_l,$opt_q);
my $limit = $opt_l || 1000;

foreach my $sub (@ARGV) {

    # Create the directory if necessary.
    our($url,@links,$directory);
    our $count = 0;
    if($opt_u){
	    $directory = "user_".$sub;
	    $url = "http://www.reddit.com/user/$sub/.json?limit=100"
    }
    elsif($opt_q){
        $sub =~ s/\s/+/g;
        $directory = "search_".$sub;
        $url = "http://www.reddit.com/search/.json?q=$sub&limit=100";
        print "$url\n";
    }
    else{
	    $directory = $sub;
	    $url = "http://www.reddit.com/r/$sub/.json?limit=100";
    }
    mkdir $directory unless -e $directory;
    do {
        $mech->get($url);
        my $json_text = JSON->new->allow_nonref->utf8->relaxed->decode($mech->text);

        # pull in all non-self posts
        # we could also add a filter for only imgur domains
        # but that depends on what sites we intend to scrape from
        my @posts = grep { !$_->{data}{is_self} } @{$json_text->{data}->{children}};
        foreach my $post (@posts) {
            my $uri = URI->new($post->{data}->{url}) or next;
            ## remove any paramters such as http://i.imgur.com/K7EdYrV.jpg?1
            $uri->query_form({});
            ## remove uri fragments like #foo
            $uri->fragment(undef);
            push @links, get_image_links($uri->as_string) if @links < $limit;
        }
        # get next page
        $url = get_next_page($json_text->{data}{after}, $url,$count);
   	    sleep INTERVAL;
    }
    while($url and @links < $limit);

    if($opt_u){
        print "Extracted ".@links." images from user $sub\n";
    } elsif($opt_q){
        print "Extracted ".@links." images from the search: $sub\n";
    } else {
	    print 'Extracted ' . @links . " images for download from r/$sub\n";
    }
    foreach my $img (@links) {
        my $album_dir = $img->{album_hash} || '';
        my $file_name = $img->{file_name};
        mkdir "$directory/$album_dir" unless -e "$directory/$album_dir";
        my $path = join '/', grep $_, ($directory, $album_dir, $file_name);
        unless (-e $path) {
            print 'Downloading ' . $img->{url} . " to $path\n";
            $mech->get($img->{url}, ':content_file' => "$path");
            sleep INTERVAL;
        }
	    else{
		    print "File already exists..." . $img->{file_name} . "\n";
	    }
    }
    undef @links;
}

sub get_next_page {
    my ($after, $url) = @_;
    return undef unless $after;
    my $uri  = URI->new($url);
    my %form = $uri->query_form;
    my $count = $_[2];
    $_[2] = $_[2] + 100;
    if($opt_u){
	 $uri->query_form({limit =>$form{limit},after => $after,count=>$count});
    } else {
    	$uri->query_form({limit => $form{limit}, after => $after});
    }
    return $uri->as_string;
}

sub get_image_links {
    my ($url) = @_;
    my @images;
    if (my ($album) = $url =~ m{ imgur\.com/a/ ([^/]+) /?$ }ix) {
        $mech->get($url);
        my ($json) = ($mech->content =~ m{ \b images \s* : \s* (.+?) , \s* cover \s* : }isx);
        my $struct = eval { $json && JSON->new->allow_nonref->utf8->relaxed->decode($json) };
        if ($struct) {
            foreach my $item (@{$struct->{items}}) {
                my %info = (album_hash => $album);
                $info{file_name} = $item->{hash} . $item->{ext};
                $info{url} = 'http://imgur.com/' . $info{file_name};
                push @images, \%info if $info{url} && $info{file_name};
            }
        }
        $mech->back;
    }
    elsif ($url =~ m{ imgur\.com/ ([^/\.]+) (\. [^/\.]+)? /?$ }ix) {
        my %info;
        unless ($2) {
            $mech->follow_link( url_regex => qr/$1/i );
            $url = $mech->uri;
        }
        $info{url} = $url;
        ($info{file_name}) = $url =~ m{ imgur\.com/([^/]+) }ix;
        push @images, \%info if $info{url} && $info{file_name};
    }
    return @images;
}

sub HELP_MESSAGE {
    print "Flags\n";
    print "\t-l limit : limit the number of pictures to download. Default: 1000\n";
    print "\t-u : process a list of users instead of a list of subreddits.\n";
    print "\t-q : process a list of search strings. Strings may contain spaces but these must be escaped with \\ or enclosed in \"\"\n"
}

sub VERSION_MESSAGE {
    print "Reddit-Image-Scrapper version ".$version_string."\n";
}
